{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "assert(int(str('').join(torch.__version__.split('.')[0:2])) >= 13) # requires at least pytorch version 1.3.0\n",
    "\n",
    "torch.set_grad_enabled(False) # make sure to not compute gradients for computational performance\n",
    "\n",
    "torch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance\n",
    "\n",
    "#bsds500 dataset for contour detection\n",
    "arguments_strModel = 'bsds500'\n",
    "\n",
    "#Iterates through all image files in the input folder\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append([filename,img])\n",
    "    return images\n",
    "\n",
    "#INPUT FOLDER\n",
    "input_folder='C:/Users/cheth/Desktop/Capstone/HED and Shape detection/input' \n",
    "\n",
    "#OUTPUT FOLDER\n",
    "output_folder = \"C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\" \n",
    "\n",
    "#Read all images within the specified folder. Store in image_list\n",
    "image_list = load_images_from_folder(input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\n",
    "\t\tself.netVggOne = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.netVggTwo = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.netVggThr = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.netVggFou = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.netVggFiv = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False),\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\ttorch.nn.ReLU(inplace=False)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.netScoreOne = torch.nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.netScoreTwo = torch.nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.netScoreThr = torch.nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.netScoreFou = torch.nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.netScoreFiv = torch.nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\t\tself.netCombine = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Conv2d(in_channels=5, out_channels=1, kernel_size=1, stride=1, padding=0),\n",
    "\t\t\ttorch.nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.load_state_dict({ strKey.replace('module', 'net'): tenWeight for strKey, tenWeight in torch.hub.load_state_dict_from_url(url='http://content.sniklaus.com/github/pytorch-hed/network-' + arguments_strModel + '.pytorch', file_name='hed-' + arguments_strModel).items() })\n",
    "\t# end\n",
    "\n",
    "\tdef forward(self, tenInput):\n",
    "\t\ttenBlue = (tenInput[:, 0:1, :, :] * 255.0) - 104.00698793\n",
    "        \n",
    "\t\ttenGreen = (tenInput[:, 1:2, :, :] * 255.0) - 116.66876762\n",
    "\t\ttenRed = (tenInput[:, 2:3, :, :] * 255.0) - 122.67891434\n",
    "\n",
    "\t\ttenInput = torch.cat([ tenBlue, tenGreen, tenRed ], 1)\n",
    "\n",
    "\t\ttenVggOne = self.netVggOne(tenInput)\n",
    "\t\ttenVggTwo = self.netVggTwo(tenVggOne)\n",
    "\t\ttenVggThr = self.netVggThr(tenVggTwo)\n",
    "\t\ttenVggFou = self.netVggFou(tenVggThr)\n",
    "\t\ttenVggFiv = self.netVggFiv(tenVggFou)\n",
    "\n",
    "\t\ttenScoreOne = self.netScoreOne(tenVggOne)\n",
    "\t\ttenScoreTwo = self.netScoreTwo(tenVggTwo)\n",
    "\t\ttenScoreThr = self.netScoreThr(tenVggThr)\n",
    "\t\ttenScoreFou = self.netScoreFou(tenVggFou)\n",
    "\t\ttenScoreFiv = self.netScoreFiv(tenVggFiv)\n",
    "\n",
    "\t\ttenScoreOne = torch.nn.functional.interpolate(input=tenScoreOne, size=(tenInput.shape[2], tenInput.shape[3]), mode='bilinear', align_corners=False)\n",
    "\t\ttenScoreTwo = torch.nn.functional.interpolate(input=tenScoreTwo, size=(tenInput.shape[2], tenInput.shape[3]), mode='bilinear', align_corners=False)\n",
    "\t\ttenScoreThr = torch.nn.functional.interpolate(input=tenScoreThr, size=(tenInput.shape[2], tenInput.shape[3]), mode='bilinear', align_corners=False)\n",
    "\t\ttenScoreFou = torch.nn.functional.interpolate(input=tenScoreFou, size=(tenInput.shape[2], tenInput.shape[3]), mode='bilinear', align_corners=False)\n",
    "\t\ttenScoreFiv = torch.nn.functional.interpolate(input=tenScoreFiv, size=(tenInput.shape[2], tenInput.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "\t\treturn self.netCombine(torch.cat([ tenScoreOne, tenScoreTwo, tenScoreThr, tenScoreFou, tenScoreFiv ], 1))\n",
    "\t# end\n",
    "# end\n",
    "\n",
    "netNetwork = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate(tenInput):\n",
    "\tglobal netNetwork\n",
    "\n",
    "\tif netNetwork is None:\n",
    "\t\tnetNetwork = Network().cuda().eval()\n",
    "\t# end\n",
    "\n",
    "\tintWidth = tenInput.shape[2]\n",
    "\tintHeight = tenInput.shape[1]\n",
    "\n",
    "\t#assert(intWidth == 480) # remember that there is no guarantee for correctness, comment this line out if you acknowledge this and want to continue\n",
    "\t#assert(intHeight == 320) # remember that there is no guarantee for correctness, comment this line out if you acknowledge this and want to continue\n",
    "\n",
    "\treturn netNetwork(tenInput.cuda().view(1, 3, intHeight, intWidth))[0, :, :, :].cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\brokegirls_original.png\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\broke_girls.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\circles.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\circular.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\city.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\ddlj.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\devil_wears_prada.png\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\home_alone.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\office.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\rect.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\room.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\tbbt.jpg\n",
      "C:/Users/cheth/Desktop/Capstone/HED and Shape detection/hed_output\\young_sheldon.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Iterating through all images and passing it for edge detection\n",
    "for i in image_list:\n",
    "    arguments_strIn = os.path.join(input_folder,i[0])\n",
    "    arguments_strOut =  os.path.join(output_folder,i[0])\n",
    "    print(arguments_strOut)\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        tenInput = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strIn))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n",
    "\n",
    "        tenOutput = estimate(tenInput)\n",
    "\n",
    "        PIL.Image.fromarray((tenOutput.clamp(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, 0] * 255.0).astype(numpy.uint8)).save(arguments_strOut)\n",
    "        \n",
    "        #Free-ing GPU memory\n",
    "        del tenOutput\n",
    "        del tenInput\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewritting the output image with inverted colors\n",
    "\n",
    "images = load_images_from_folder(output_folder)\n",
    "\n",
    "for i in images:\n",
    "    arguments_strIn = os.path.join(output_folder,i[0])\n",
    "    image = Image.open(arguments_strIn)\n",
    "    arguments_strOut =  os.path.join(output_folder,i[0])\n",
    "    inverted_image = PIL.ImageOps.invert(image)\n",
    "    inverted_image.save(arguments_strOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stackImages(scale,imgArray):\n",
    "    rows = len(imgArray)\n",
    "    cols = len(imgArray[0])\n",
    "    rowsAvailable = isinstance(imgArray[0], list)\n",
    "    width = imgArray[0][0].shape[1]\n",
    "    height = imgArray[0][0].shape[0]\n",
    "    if rowsAvailable:\n",
    "        for x in range ( 0, rows):\n",
    "            for y in range(0, cols):\n",
    "                if imgArray[x][y].shape[:2] == imgArray[0][0].shape [:2]:\n",
    "                    imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale)\n",
    "                else:\n",
    "                    imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]), None, scale, scale)\n",
    "                if len(imgArray[x][y].shape) == 2: imgArray[x][y]= cv2.cvtColor( imgArray[x][y], cv2.COLOR_GRAY2BGR)\n",
    "        imageBlank = np.zeros((height, width, 3), np.uint8)\n",
    "        hor = [imageBlank]*rows\n",
    "        hor_con = [imageBlank]*rows\n",
    "        for x in range(0, rows):\n",
    "            hor[x] = np.hstack(imgArray[x])\n",
    "        ver = np.vstack(hor)\n",
    "    else:\n",
    "        for x in range(0, rows):\n",
    "            if imgArray[x].shape[:2] == imgArray[0].shape[:2]:\n",
    "                imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale)\n",
    "            else:\n",
    "                imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None,scale, scale)\n",
    "            if len(imgArray[x].shape) == 2: imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR)\n",
    "        hor= np.hstack(imgArray)\n",
    "        ver = hor\n",
    "    return ver\n",
    "\n",
    "def getcontours(img):\n",
    "    contours,hierarchy=cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    for cnt in contours:\n",
    "        area=cv2.contourArea(cnt)\n",
    "        #Shape detection block\n",
    "        if area>500:\n",
    "           cv2.drawContours(imgcontour, cnt, -1, (255, 0, 0), 3)\n",
    "           peri=cv2.arcLength(cnt,True)\n",
    "          \n",
    "           approx=cv2.approxPolyDP(cnt,0.02*peri,True)\n",
    "           objcolor=len(approx)\n",
    "           x,y,w,h=cv2.boundingRect(approx)\n",
    "           if objcolor==3:objectType=\"Triangle\"\n",
    "           elif objcolor==4:\n",
    "               aspratio=w/float(h)\n",
    "               if aspratio>0.95 and aspratio<1.05:objectType=\"square\"\n",
    "               else:objectType=\"rectangle\"\n",
    "           elif objcolor==5:objectType=\"pentagon\"\n",
    "           elif objcolor==6:objectType=\"hexagon\"\n",
    "           elif objcolor==7:objectType=\"heptagon\"\n",
    "           #elif objcolor==8:objectType=\"octagon\"\n",
    "           elif objcolor>10:objectType=\"circle\"\n",
    "           else:objectType=\"None\"\n",
    "           #order => objecttype, area, perimeter , num of sides , x cordinate , y coordinate , width , height \n",
    "           img_details.append([objectType, area , peri, len(approx), x , y, w ,h ])\n",
    "           #Drawing bounding boxes for detected shapes\n",
    "           cv2.rectangle(imgcontour,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "           #cv2.putText(imgcontour,objectType,(x+(w//2)-5,y+(h//2)-10),cv2.FONT_HERSHEY_COMPLEX,0.5,(0,0,0),2)\n",
    "        \n",
    "    \n",
    "    sorted_list = sorted(img_details,  key=lambda x: x[1], reverse = True)\n",
    "    print(\"\\n\")\n",
    "    #print(sorted_list)\n",
    "    return sorted_list\n",
    "\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'img_id': 0, 'img_name': 'brokegirls_original.png', 'Tags': [], 'Region1': [270, 257, 160, 78], 'Region2': [239, 66, 171, 96]}\n",
      "\n",
      "\n",
      "{'img_id': 1, 'img_name': 'broke_girls.jpg', 'Tags': [], 'Region1': [16, 0, 74, 177], 'Region2': [223, 184, 25, 38]}\n",
      "\n",
      "\n",
      "{'img_id': 2, 'img_name': 'circles.jpg', 'Tags': [], 'Region1': [322, 257, 155, 155], 'Region2': [359, 9, 135, 135]}\n",
      "\n",
      "\n",
      "{'img_id': 3, 'img_name': 'circular.jpg', 'Tags': [], 'Region1': [126, 16, 69, 78], 'Region2': [140, 130, 38, 19]}\n",
      "\n",
      "\n",
      "{'img_id': 4, 'img_name': 'city.jpg', 'Tags': [], 'Region1': [210, 6, 49, 130], 'Region2': [253, 336, 30, 44]}\n",
      "\n",
      "\n",
      "{'img_id': 5, 'img_name': 'ddlj.jpg', 'Tags': [], 'Region1': [176, 163, 135, 4], 'Region2': []}\n",
      "\n",
      "\n",
      "{'img_id': 6, 'img_name': 'devil_wears_prada.png', 'Tags': [], 'Region1': [504, 177, 96, 102], 'Region2': [469, 8, 70, 90]}\n",
      "\n",
      "\n",
      "{'img_id': 7, 'img_name': 'home_alone.jpg', 'Tags': [], 'Region1': [199, 113, 41, 57], 'Region2': []}\n",
      "\n",
      "\n",
      "{'img_id': 8, 'img_name': 'office.jpg', 'Tags': [], 'Region1': [185, 179, 73, 184], 'Region2': [111, 358, 121, 87]}\n",
      "\n",
      "\n",
      "{'img_id': 9, 'img_name': 'rect.jpg', 'Tags': [], 'Region1': [565, 23, 125, 154], 'Region2': [314, 22, 114, 153]}\n",
      "\n",
      "\n",
      "{'img_id': 10, 'img_name': 'room.jpg', 'Tags': [], 'Region1': [51, 56, 18, 46], 'Region2': []}\n",
      "\n",
      "\n",
      "{'img_id': 11, 'img_name': 'tbbt.jpg', 'Tags': [], 'Region1': [198, 628, 102, 87], 'Region2': [859, 231, 62, 106]}\n",
      "\n",
      "\n",
      "{'img_id': 12, 'img_name': 'young_sheldon.png', 'Tags': [], 'Region1': [65, 56, 30, 48], 'Region2': [282, 197, 48, 33]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "images = load_images_from_folder(output_folder)\n",
    "img_details_final = dict()\n",
    "count = 0\n",
    "for i in images: \n",
    "   img_details=[]\n",
    "   img_details_final['img_id'] = count\n",
    "   count = count + 1 \n",
    "   img_name = i[0]\n",
    "   img_details_final['img_name'] = img_name \n",
    "   img_details_final['Tags']  = []\n",
    "   img_details_final['Region1'] = []\n",
    "   img_details_final['Region2'] = []\n",
    "   \n",
    "   \n",
    "   writting_file = 'C:/Users/cheth/Desktop/Capstone/HED and Shape detection/shape_output'\n",
    "   arguments_strIn = os.path.join(output_folder,i[0])\n",
    "   writting_file = os.path.join(writting_file,i[0])\n",
    "   \n",
    "   arguments_strOut =  os.path.join(writting_file,i[0])\n",
    "   \n",
    "   img=cv2.imread(arguments_strIn)\n",
    "   imgcontour=img.copy()\n",
    "   imggray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "   imgblur=cv2.GaussianBlur(imggray,(7,7),1)\n",
    "   imgcanny=cv2.Canny(imgblur,50,50)\n",
    "   region_data = getcontours(imgcanny)\n",
    "   \n",
    "   if len(region_data)>0:\n",
    "       img_details_final['Region1'] = [region_data[0][4],region_data[0][5],region_data[0][6],region_data[0][7]]\n",
    "   if len(region_data)>=2:\n",
    "       img_details_final['Region2'] = [region_data[1][4],region_data[1][5],region_data[1][6],region_data[1][7]]\n",
    "   print(img_details_final)   \n",
    "   \n",
    "   imgblank=np.zeros_like(img)\n",
    "   mask = np.zeros(img.shape[:2],np.uint8)\n",
    "\n",
    "\n",
    "   imgstack=stackImages(0.8,[[img,imggray,imgblur],[imgcanny,imgcontour,imgblank]])\n",
    "   #cv2.imshow(\"all\",imgcontour)\n",
    "   cv2.imwrite(writting_file,imgcontour)\n",
    "   \n",
    "   #Writting output into json file\n",
    "   import json \n",
    "   with open(\"C:/Users/cheth/Desktop/Capstone/HED and Shape detection/regions_new.ndjson\", \"w\") as outfile:\n",
    "       for rec in img_details_final:\n",
    "           json.dump(rec, outfile)\n",
    "           outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "   outfile.close()\n",
    "   cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
